package Demo

import NewTest.utils.PropertiesReaderUtils
import kafka.serializer.StringDecoder
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils

import scala.collection.mutable

/**
  * Created by liuxiang on 2018/12/10.
  */
object LinesDemo {
  def main(args: Array[String]): Unit = {
    //创建sparkConf
    val sparkConf: SparkConf = new SparkConf()
      .setAppName("LinesDemo")
    //创建sparkContext
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    //创建StreamingContext
    val ssc = new StreamingContext(sc, Seconds(5))
    //获取读取配置文件工具类
    val prop = PropertiesReaderUtils.getProperties("/conf/path.properties")
    //配置kafka相关参数
    val kafkaParams = Map("metadata.broker.list" -> prop.getProperty("metadata.broker.list"))
    //定义topic
    val topics = Set(prop.getProperty("topic.picture"))
    //通过 KafkaUtils.createDirectStream接受kafka数据，这里采用是kafka低级api偏移量不受zk管理
    val dstream: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)
    //获取kafka中topic中的数据
    val topicData: DStream[String] = dstream.map(_._2)

    //获取查询数据的总条数
    val dataWithLines:DStream[(String, String, String, Long)] = topicData.map(t => {
      val uuid = t.split(" ")(0)
      val querySentence = t.split(" ")(1)
      val sourceFeature = t.split(" ")(2)
      //获取总条数
      (uuid, querySentence, sourceFeature, 217)
    })

    //将查询数据分割为50份
    val splitData = dataWithLines.flatMap(x => {
      val uuid = x._1
      val querySentence = x._2
      val sourceFeature = x._3
      val lines = x._4.toInt
      //设置查询的起始点和终止点
      val foot = lines / 50 + 1
      for (i <- 1 until lines by foot) yield {
        var start = i
        println("start" + start)
        var end = start + foot - 1
        println("end" + end)
        (uuid, querySentence, sourceFeature, start, end)
      }
    })

    //分批查询solr中的特征向量
    val solrFeatures = splitData.map(t => {
      val uuid = t._1
      val querySentence = t._2
      val sourceFeature = t._3
      val startLine = t._4
      val endLine = t._5
      //查询所得的数据数组
      val temp = mutable.Buffer[String]();
      temp += ("hive","flume","sqoop","oozie");
      val targetFeatures:mutable.Buffer[String] = temp
      (uuid, sourceFeature, targetFeatures)
    })

    //计算余弦距离
    val cosDist:DStream[(String, mutable.Buffer[Double])] = solrFeatures.map(u => {
      val uuid = u._1
      val sourceFeature = u._2
      val targetFeatures = u._3
      //对数组进行遍历计算cosin值
      val featureDist = targetFeatures.map(feature => {
        10.toDouble
      })
      (uuid, featureDist)
    })

    //对余弦距离进行扁平化处理
    val flatDist = cosDist.flatMap(m => {
      for (i <- 0 until m._2.length) yield (m._1, m._2(i))
    })

    //对距离由大到小进行排序并取出top10
    val sortedDist = flatDist.foreachRDD(rdd => {
      //获取数据中的top10
      val orderedRdd = rdd.top(3)(Ordering.by(e => e._2))

      //插入数据
      orderedRdd.foreach(record => {
        println("record1:" + record._1)
        println("record2:" + record._2)
      })
    })

    //开启计算
    ssc.start()
    ssc.awaitTermination()
  }
}
