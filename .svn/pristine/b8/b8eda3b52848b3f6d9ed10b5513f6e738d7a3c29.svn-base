package Api.test

import Api.entity.QueryEntity
import Api.utils._
import kafka.serializer.StringDecoder
import org.apache.spark.rdd.RDD
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.JavaConversions._
import scala.collection.mutable

/**
  * 调用单个连接池,对求距离和topK的流程进行优化,更换存储介质
  * Created by liuxiang on 2018/12/10.
  */
object ChangeStorageDemo4 {
  def main(args: Array[String]): Unit = {
    //创建sparkConf
    val sparkConf: SparkConf = new SparkConf()
      .setAppName("ChangeStorageDemo4")
    //创建sparkContext
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    //获取读取配置文件工具类
    val prop = PropertiesReaderUtils.getProperties("conf/path.properties")
    //读取配置文件
    val window = prop.getProperty("picture.window").toInt
    val split = prop.getProperty("picture.split").toInt
    val top = prop.getProperty("picture.top").toInt
    val query = prop.getProperty("picture.query")
    //创建StreamingContext
    val ssc = new StreamingContext(sc, Seconds(window))
    //配置kafka相关参数
    val kafkaParams = Map("metadata.broker.list" -> prop.getProperty("metadata.broker.list"))
//    val kafkaParams = Map("metadata.broker.list"-> prop.getProperty("metadata.broker.list"),"group.id" -> prop.getProperty("metadata.group.id"))
    //定义topic
    val topics = Set(prop.getProperty("topic.picture"))
    //通过 KafkaUtils.createDirectStream接受kafka数据，这里采用是kafka低级api偏移量不受zk管理
    val dstream: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)
    //获取kafka中topic中的数据
    val topicData: DStream[String] = dstream.map(_._2)

    //将数据预处理为包含uuid，uuid, objType, sourceFeature, targetFeatures的四元组
    val solrFeatures = topicData.map(t => {
      val uuid = t.split(" ")(0)
      val objType = t.split(" ")(1).toInt
      val querySentence = t.split(" ")(2)
      val sourceFeature = t.split(" ")(3)
      val startLine = t.split(" ")(4).toInt
      val endLine = t.split(" ")(5).toInt
      //查询所得的数据数组
      val targetFeatures:mutable.Buffer[QueryEntity] = SolrQueryUtils.solrQuery(startLine, endLine, querySentence)
      (uuid, objType, sourceFeature, targetFeatures)
    })

    //对数据进行扁平化处理,包含uuid, objType, sourceFeature, QueryEntity
    val flatSolrFeatures = solrFeatures.flatMap(m => {
      for (i <- 0 until m._4.length) yield (m._1, m._2, m._3, m._4(i))
    }).map(iter => {
      val uuid = iter._1
      val objType = iter._2
      val sourceFeature = iter._3
      val targetFeature = iter._4
      val id = targetFeature.getId
      val feature = targetFeature.getFeature
      val dist = CompareFeatureUtils.compare(objType, sourceFeature, feature)
      val valueMap = new mutable.HashMap[String, Double]
      (uuid, (id, dist))
    })

    //对距离由大到小进行排序并取出top10
    val sortedDist = flatSolrFeatures.foreachRDD(rdd => {
      val sortedData:RDD[(Double, (String, String))] = rdd.map{ case (k, v) => (v._2, (k, v._1))}.sortByKey(false)
      val topKData = sortedData.take(top).map{ case (k, v) => (v._1, v._2, k)}

//      val con = JdbcConnectUtils.getInstance.getConnection
//      val ps = con.prepareStatement(query)

      sortedData.saveAsObjectFile("/user/hadoop/output")
      //插入数据
//      topKData.foreach(record => {

//        ps.setString(1, record._1)
//        ps.setString(2, record._2)
//        ps.setDouble(3, record._3)
//        ps.execute()
//      })

//      ps.close()
//      con.close()
    })

    //开启计算
    ssc.start()
    ssc.awaitTermination()
  }
}
