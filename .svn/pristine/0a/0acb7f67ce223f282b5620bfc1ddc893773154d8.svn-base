package com.qst.work

import com.alibaba.fastjson.{JSON, JSONObject}
import com.qst.utils.{FastJsonUtils, HbaseUtils, PropertiesUtils}
import kafka.serializer.StringDecoder
import org.apache.hadoop.hbase.client.Put
import org.apache.spark.SparkConf
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.slf4j.LoggerFactory

import scala.collection.mutable

/**
  * created by hkl
  */

class KafkaToHbaseWork {}

object KafkaToHbaseWork {

  private val logger = LoggerFactory.getLogger(classOf[KafkaToHbaseWork])
  val prop = PropertiesUtils.load("conf/configuration.properties")

  def main(args: Array[String]): Unit = {

    val conf = new SparkConf().setAppName("json_test")
    conf.set("spark.testing.memory", "2147480000") // 后面的值大于512m即可
    val ssc = new StreamingContext(conf, Seconds(4))

    //get configuration
    val kafkaParams = Map("metadata.broker.list" -> prop.getProperty("broker.list"), "group.id" -> prop.getProperty("group"))
    val topics = Set(prop.getProperty("topics"))
    val tablename = prop.getProperty("table")
    val columnFamilys = prop.getProperty("ColumnFamilyName").split(",")
    val objtypes = prop.getProperty("ObjTypes").split(",")

    //原始数据
    val lines = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics).map(_._2)

    // jsonstring -> jsonobject
    val jsonobjs = lines.flatMap { x =>
      var buffer = mutable.ArrayBuffer[JSONObject]()
      //筛选掉不是合格的json字符串的情况
      if (FastJsonUtils.isLegal(x)) {
        var jsonobject = JSON.parseObject(x)
        buffer.append(jsonobject)
      } else {
        logger.error("jsonString is not correct")
      }
      buffer
    }
    // 缓存也可以根据实际业务保存,也可以用cache,cache只支持MEMORY_ONLY级别缓存
    val words = jsonobjs.persist

    //将数据分流
    for (i <- 0 to columnFamilys.length - 1) {
      var words_1 = words.filter(x => {
        objtypes(i).equals(x.get("objType").toString)
      })

      words_1.foreachRDD { x =>
        var puts = mutable.ArrayBuffer[Put]()
        val arr = x.collect()
        for (t <- arr) {
          var put = FastJsonUtils.dataAnalysis(t, columnFamilys(i))
          puts.append(put)
        }
        if (puts.size > 0) {
          println("数组大小" + puts.size)
          HbaseUtils.put(tablename, scala.collection.JavaConversions.bufferAsJavaList(puts))
        }
      }
    }

    ssc.start()
    ssc.awaitTermination()
  }

}
