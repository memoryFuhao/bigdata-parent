package Demo

import NewTest.utils.{ConnectPoolUtil, DemoUtils, PropertiesReaderUtils, QueryEntity}
import cn.jiuling.jni.QstCompareFeatureApiTest
import kafka.serializer.StringDecoder
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.JavaConversions._
import scala.collection.mutable

/**
  * Created by liuxiang on 2018/12/10.
  */
object EndDemo {
  def main(args: Array[String]): Unit = {
    //创建sparkConf
    val sparkConf: SparkConf = new SparkConf()
      .setAppName("EndDemo")
    //创建sparkContext
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    //获取读取配置文件工具类
    val prop = PropertiesReaderUtils.getProperties("/conf/path.properties")
    //读取配置文件
    val window = prop.getProperty("picture.window").toInt
    val split = prop.getProperty("picture.split").toInt
    val top = prop.getProperty("picture.top").toInt
    val query = prop.getProperty("picture.query")
    //加载sdk
    System.load(prop.getProperty("qst.sdk1"))
    System.load(prop.getProperty("qst.sdk2"))
    //创建StreamingContext
    val ssc = new StreamingContext(sc, Seconds(window))
    //配置kafka相关参数
    val kafkaParams = Map("metadata.broker.list" -> prop.getProperty("metadata.broker.list"))
    //定义topic
    val topics = Set(prop.getProperty("topic.picture"))
    //通过 KafkaUtils.createDirectStream接受kafka数据，这里采用是kafka低级api偏移量不受zk管理
    val dstream: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)
    //获取kafka中topic中的数据
    val topicData: DStream[String] = dstream.map(_._2)

    //获取查询数据的总条数
    val dataWithLines:DStream[(String, String, String, Long)] = topicData.map(t => {
      val uuid = t.split(" ")(0)
      val querySentence = t.split(" ")(1)
      val sourceFeature = t.split(" ")(2)
      //获取总条数
      val lines = DemoUtils.getDataLines(querySentence)
      (uuid, querySentence, sourceFeature, lines)
    })

    //将查询数据分割为50份
    val splitData = dataWithLines.flatMap(x => {
      val uuid = x._1
      val querySentence = x._2
      val sourceFeature = x._3
      val lines = x._4.toInt
      //设置查询的起始点和终止点
      val foot = lines / split + 1
      for (i <- 1 until lines by foot) yield {
        val start = i
//        println("start" + start)
        val end = start + foot - 1
//        println("end" + end)
        (uuid, querySentence, sourceFeature, start, end)
      }
    })

    //分批查询solr中的特征向量
    val solrFeatures = splitData.map(t => {
      val uuid = t._1
      val querySentence = t._2
      val sourceFeature = t._3
      val startLine = t._4
      val endLine = t._5
      //查询所得的数据数组
      val targetFeatures:mutable.Buffer[QueryEntity] = DemoUtils.combinationQuery1(startLine, endLine, querySentence)
      (uuid, sourceFeature, targetFeatures)
    })

    //计算余弦距离
    val cosDist:DStream[(String, mutable.Buffer[(String, String, Double)])] = solrFeatures.map(u => {
      val uuid = u._1
      val sourceFeature = u._2
      val targetFeatures = u._3
      //对数组进行遍历计算cosin值
      val featureDist = targetFeatures.map(feature => {
        //对数据进行切分
        val id = feature.getId
        val fea = feature.getFeature
        val url = feature.getUrl
        val dist = QstCompareFeatureApiTest.main(fea, sourceFeature)
        (id, url, dist)
//        println("tempCos:" + temp)
      })
      (uuid, featureDist)
    })

    //对余弦距离进行扁平化处理
    val flatDist = cosDist.flatMap(m => {
      for (i <- 0 until m._2.length) yield (m._1, m._2(i))
    })

    //对距离由大到小进行排序并取出top10
    val sortedDist = flatDist.foreachRDD(rdd => {
      //获取数据中的top10
      val orderedRdd = rdd.top(top)(Ordering.by(e => e._2._2))
      val con = ConnectPoolUtil.getConnection()
      val ps = con.prepareStatement(query)

      //插入数据
      orderedRdd.foreach(record => {
        ps.setString(1, record._1)
        ps.setString(2, record._2._1)
        ps.setString(3, record._2._2)
        ps.setDouble(4, record._2._3.toDouble)
        ps.execute()
      })

      ConnectPoolUtil.closeConn(ps, con)
    })

    //开启计算
    ssc.start()
    ssc.awaitTermination()
  }
}
