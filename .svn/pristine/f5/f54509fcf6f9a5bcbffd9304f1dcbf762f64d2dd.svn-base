package Api.test

import Api.entity.QueryEntity
import Api.utils._
import kafka.serializer.StringDecoder
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.JavaConversions._
import scala.collection.mutable

/**
  * 将数据分流放在上层
  * Created by liuxiang on 2018/12/10.
  */
object SolrUpDemo1 {
  def main(args: Array[String]): Unit = {
    //创建sparkConf
    val sparkConf: SparkConf = new SparkConf()
      .setAppName("SolrUpDemo1")
    //创建sparkContext
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    //获取读取配置文件工具类
    val prop = PropertiesReaderUtils.getProperties("conf/path.properties")
    //读取配置文件
    val window = prop.getProperty("picture.window").toInt
    val split = prop.getProperty("picture.split").toInt
    val top = prop.getProperty("picture.top").toInt
    val query = prop.getProperty("picture.query")
    //创建StreamingContext
    val ssc = new StreamingContext(sc, Seconds(window))
    //配置kafka相关参数
    val kafkaParams = Map("metadata.broker.list" -> prop.getProperty("metadata.broker.list"))
//    val kafkaParams = Map("metadata.broker.list"-> prop.getProperty("metadata.broker.list"),"group.id" -> prop.getProperty("metadata.group.id"))
    //定义topic
    val topics = Set(prop.getProperty("topic.picture"))
    //通过 KafkaUtils.createDirectStream接受kafka数据，这里采用是kafka低级api偏移量不受zk管理
    val dstream: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)
    //获取kafka中topic中的数据
    val topicData: DStream[String] = dstream.map(_._2)

    //分批查询solr中的特征向量
    val solrFeatures = topicData.map(t => {
      println(t)
      val uuid = t.split(" ")(0)
      val objType = t.split(" ")(1).toInt
      val querySentence = t.split(" ")(2)
      val sourceFeature = t.split(" ")(3)
      val startLine = t.split(" ")(4).toInt
      val endLine = t.split(" ")(5).toInt
      //查询所得的数据数组
      val targetFeatures:mutable.Buffer[QueryEntity] = SolrQueryUtils.solrQuery(startLine, endLine, querySentence)
      (uuid, objType, sourceFeature, targetFeatures)
    })

    //计算余弦距离
    val cosDist:DStream[(String, mutable.Buffer[(String,  Double)])] = solrFeatures.map(u => {
      val uuid = u._1
      val objType = u._2
      val sourceFeature = u._3
      val targetFeatures = u._4
      //对数组进行遍历计算cosin值
      val featureDist = targetFeatures.map(feature => {
        //对数据进行切分
        val id = feature.getId
        val fea = feature.getFeature
        val dist = CompareFeatureUtils.compare(objType, fea, sourceFeature)
        (id, dist)
      })
      (uuid, featureDist)
    })

    //对余弦距离进行扁平化处理
    val flatDist = cosDist.flatMap(m => {
      for (i <- 0 until m._2.length) yield (m._1, m._2(i))
    })

    //对距离由大到小进行排序并取出top10
    val sortedDist = flatDist.foreachRDD(rdd => {

//      //获取数据中的top10
//      val orderedRdd = rdd.groupByKey().map(t => {
//        val uuid = t._1
//        val featureDist = t._2
//        var lists = List[(String, String, Double)]()
//        for (i <- featureDist) {
//          val id = i._1
//          val url = i._2
//          val dis = i._3
//          lists = ((id, url, dis)) :: lists
//        }
//        lists.sortBy(_._3).reverse.take(top)
//        new Tuple2[String, Iterable[(String, String, Double)]](uuid, lists)
//      })
//
//      //插入数据库
//      val con = ConnectPoolUtil.getConnection()
//      val ps = con.prepareStatement(query)
//      orderedRdd.foreach(record => {
//        ps.setString(1, record._1)
//        val temp = record._2
//        for (i <- temp) {
//          ps.setString(2, i._1)
//          ps.setString(3, i._2)
//          ps.setDouble(4, i._3)
//        }
//        ps.execute()
//      })
//
//      ConnectPoolUtil.closeConn(ps, con)

      val orderedRdd = rdd.top(top)(Ordering.by(e => e._2._2))
      val con = ConnectPoolUtil.getConnection()
      val ps = con.prepareStatement(query)

      //插入数据
      orderedRdd.foreach(record => {
        ps.setString(1, record._1)
        ps.setString(2, record._2._1)
        ps.setDouble(3, record._2._2.toDouble)
        ps.execute()
      })

      ConnectPoolUtil.closeConn(ps, con)
    })

    //开启计算
    ssc.start()
    ssc.awaitTermination()
  }
}
