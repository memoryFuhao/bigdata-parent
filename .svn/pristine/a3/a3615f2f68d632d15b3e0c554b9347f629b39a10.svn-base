package NewTest.sparkstreaming

import NewTest.utils.{CombinationQueryUtils, ConnectPoolUtil, PropertiesReaderUtils}
import cn.jiuling.jni.QstCompareFeatureApiTest
import kafka.serializer.StringDecoder
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.JavaConversions._
import scala.collection.mutable

/**
  * Created by liuxiang on 2018/12/4.
  */
object CalculateCosin {
  def main(args: Array[String]): Unit = {
    //创建sparkConf
    val sparkConf: SparkConf = new SparkConf()
      .setAppName("CalculateCosin")
      .setMaster("local[4]")
    //创建sparkContext
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    //创建StreamingContext
    val ssc = new StreamingContext(sc, Seconds(5))
    //获取读取配置文件工具类
    val prop = PropertiesReaderUtils.getProperties("/conf/path.properties")
    //配置kafka相关参数
    val kafkaParams = Map("metadata.broker.list" -> prop.getProperty("metadata.broker.list"))
    //定义topic
    val topics = Set(prop.getProperty("topic.picture"))

    //通过 KafkaUtils.createDirectStream接受kafka数据，这里采用是kafka低级api偏移量不受zk管理
    val dstream: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)

    //获取kafka中topic中的数据
    //数据格式为 uuid, querySentence, sourceFeature
    val topicData: DStream[String] = dstream.map(_._2)

    //获取查询数据的总条数
    val dataWithLines:DStream[(String, String, String, Long)] = topicData.map(t => {
      val uuid = t.split(" ")(0)
      val querySentence = t.split(" ")(1)
      val sourceFeature = t.split(" ")(2)
      //获取总条数
      val lines = CombinationQueryUtils.getDataLines(querySentence)
      (uuid, querySentence, sourceFeature, lines)
    })

    //将查询数据分割为100份
    val splitData = dataWithLines.flatMap(x => {
      val uuid = x._1
      val querySentence = x._2
      val sourceFeature = x._3
      val lines = x._4.toInt
      //设置查询的起始点和终止点
      val foot = lines / 50 + 1
      for (i <- 1 until lines by foot) yield {
        val start = i
        val end = start + foot - 1
        (uuid, querySentence, sourceFeature, start, end)
      }
    })

    //分批查询solr中的特征向量
    val solrFeatures = splitData.map(t => {
      val uuid = t._1
      val querySentence = t._2
      val sourceFeature = t._3
      val startLine = t._4
      val endLine = t._5
      //查询所得的数据数组
      val targetFeatures:mutable.Buffer[String] = CombinationQueryUtils.combinationQuery(startLine, endLine, querySentence)
      (uuid, sourceFeature, targetFeatures)
    })

    //计算余弦距离
    val cosDist:DStream[(String, mutable.Buffer[Double])] = solrFeatures.map(u => {
      val uuid = u._1
      val sourceFeature = u._2
      val targetFeatures = u._3
      //对数组进行遍历计算cosin值
      val featureDist = targetFeatures.map(feature => {
        //加载sdk
        System.load(prop.getProperty("qst.sdk1"))
        System.load(prop.getProperty("qst.sdk2"))
        QstCompareFeatureApiTest.main(feature, sourceFeature)
      })
      (uuid, featureDist)
    })

    //对余弦距离进行扁平化处理
    val flatDist = cosDist.flatMap(m => {
      for (i <- 0 until m._2.length) yield (m._1, m._2(i))
    })

    //对距离由大到小进行排序并取出top10
    val sortedDist = flatDist.foreachRDD(rdd => {
      //获取数据中的top10
      val orderedRdd = rdd.top(10)(Ordering.by(e => e._2))

      val con = ConnectPoolUtil.getConnection()
      val ps = con.prepareStatement("INSERT INTO results (uuid, distance) VALUES (?, ?)")

      //插入数据
      orderedRdd.foreach(record => {
        ps.setString(1, record._1)
        ps.setDouble(2, record._2)
        ps.execute()
      })

      ConnectPoolUtil.closeConn(ps, con)
    })

    //开启计算
    ssc.start()
    ssc.awaitTermination()
  }
}
