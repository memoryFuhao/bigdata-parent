package Api.test

import Api.entity.QueryEntity
import Api.utils._
import kafka.serializer.StringDecoder
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.{SparkConf, SparkContext}

import scala.collection.JavaConversions._
import scala.collection.mutable

/**
  * 调用单个连接池
  * Created by liuxiang on 2018/12/10.
  */
object SingleSqlDemo2 {
  def main(args: Array[String]): Unit = {
    //创建sparkConf
    val sparkConf: SparkConf = new SparkConf()
      .setAppName("SingleSqlDemo2")
    //创建sparkContext
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    //获取读取配置文件工具类
    val prop = PropertiesReaderUtils.getProperties("conf/path.properties")
    //读取配置文件
    val window = prop.getProperty("picture.window").toInt
    val split = prop.getProperty("picture.split").toInt
    val top = prop.getProperty("picture.top").toInt
    val query = prop.getProperty("picture.query")
    //创建StreamingContext
    val ssc = new StreamingContext(sc, Seconds(window))
    //配置kafka相关参数
    val kafkaParams = Map("metadata.broker.list" -> prop.getProperty("metadata.broker.list"))
//    val kafkaParams = Map("metadata.broker.list"-> prop.getProperty("metadata.broker.list"),"group.id" -> prop.getProperty("metadata.group.id"))
    //定义topic
    val topics = Set(prop.getProperty("topic.picture"))
    //通过 KafkaUtils.createDirectStream接受kafka数据，这里采用是kafka低级api偏移量不受zk管理
    val dstream: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)
    //获取kafka中topic中的数据
    val topicData: DStream[String] = dstream.map(_._2)

    //分批查询solr中的特征向量
    val solrFeatures = topicData.map(t => {
      val uuid = t.split(" ")(0)
      val objType = t.split(" ")(1).toInt
      val querySentence = t.split(" ")(2)
      val sourceFeature = t.split(" ")(3)
      val startLine = t.split(" ")(4).toInt
      val endLine = t.split(" ")(5).toInt
      //查询所得的数据数组
      val targetFeatures:mutable.Buffer[QueryEntity] = SolrQueryUtils.solrQuery(startLine, endLine, querySentence)
      (uuid, objType, sourceFeature, targetFeatures)
    })

    //计算余弦距离
    val cosDist:DStream[(String, mutable.Buffer[(String,  Double)])] = solrFeatures.map(u => {
      val uuid = u._1
      val objType = u._2
      val sourceFeature = u._3
      val targetFeatures = u._4
      //对数组进行遍历计算cosin值
      val featureDist = targetFeatures.map(feature => {
        //对数据进行切分
        val id = feature.getId
        val fea = feature.getFeature
        val dist = CompareFeatureUtils.compare(objType, fea, sourceFeature)
        println(dist)
        (id, dist)
      })
      (uuid, featureDist)
    })

    //对余弦距离进行扁平化处理
    val flatDist = cosDist.flatMap(m => {
      for (i <- 0 until m._2.length) yield (m._1, m._2(i))
    })

    //对距离由大到小进行排序并取出top10
//        val sortedDist = flatDist.foreachRDD(rdd => {
//          rdd.mapPartitions( iter => {
//            val con = JdbcConnectUtils.getInstance.getConnection
//            val ps = con.prepareStatement(query)
//            var r = null
//            while (iter.hasNext) {
//              r = iter.next
//              ps.setString(1, r.toString)
//              ps.setString(2, " ")
//              ps.setDouble(3, 1.0)
//            }
//            ps.executeBatch()
//            ps.close()
//            con.close()
//          })
//        })

    //对距离由大到小进行排序并取出top10
    val sortedDist = flatDist.foreachRDD(rdd => {
      val orderedRdd = rdd.top(top)(Ordering.by(e => e._2._2))
      val con = JdbcConnectUtils.getInstance.getConnection
      val ps = con.prepareStatement(query)

      //插入数据
      orderedRdd.foreach(record => {
        ps.setString(1, record._1)
        ps.setString(2, record._2._1)
        ps.setDouble(3, record._2._2.toDouble)
        ps.execute()
      })

      ps.close()
      con.close()
    })

    //开启计算
    ssc.start()
    ssc.awaitTermination()
  }
}
