package NewTest.sparkstreaming

import kafka.serializer.StringDecoder
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.streaming.{Seconds, StreamingContext}
import org.apache.spark.streaming.dstream.{DStream, InputDStream}
import org.apache.spark.streaming.kafka.KafkaUtils

//todo:利用sparkStreaming对接kafka实现单词计数----采用Direct(低级API)
object SparkStreamingKafka_Direct {
  def main(args: Array[String]): Unit = {
    //创建sparkConf
    val sparkConf: SparkConf = new SparkConf()
      .setAppName("SparkStreamingKafka_Direct")
    //创建sparkContext
    val sc = new SparkContext(sparkConf)
    sc.setLogLevel("WARN")
    //创建StreamingContext
    val ssc = new StreamingContext(sc, Seconds(5))
    //配置kafka相关参数
    val kafkaParams = Map("metadata.broker.list"->"master:9092,slave1:9092,slave2:9092,slave3:9092,slave4:9092","group.id"->"hkl")
//    val kafkaParams = Map("metadata.broker.list"->"master:9092,slave1:9092,slave2:9092,slave3:9092,slave4:9092")
    //定义topic
    val topics = Set("test1")
    //通过 KafkaUtils.createDirectStream接受kafka数据，这里采用是kafka低级api偏移量不受zk管理
    val dstream: InputDStream[(String, String)] = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topics)
    //获取kafka中topic中的数据
    val topicData: DStream[String] = dstream.map(_._2)
    //切分每一行,每个单词计为1
    val wordAndOne: DStream[(String, Int)] = topicData.flatMap(_.split(" ")).map((_,1))
    //相同单词出现的次数累加
    val result: DStream[(String, Int)] = wordAndOne.reduceByKey(_+_)
    //打印输出
    result.print()

    //开启计算
    ssc.start()
    ssc.awaitTermination()
  }
}